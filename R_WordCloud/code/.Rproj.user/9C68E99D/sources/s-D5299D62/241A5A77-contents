##
#   RRE - Experimental PDF auto-tag and WordCloud creation
#   Main Analysis concept from: https://mkhernandez.wordpress.com/tag/remove-special-characters/
#   Main guide from: http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know
#   Initial adaptation from: http://riccardo-hertel.com/wp/index.php/2015/09/03/using-r-to-create-a-word-cloud-from-a-pdf-document/ 
#
#   donâ€™t -> don't theyâ€™r -> they’re not removed with the stopwords process.
#   Bug explanation: http://stat545.com/block032_character-encoding.html#how-to-get-from-theyare-to-theyre
#   pdftools package reference: https://cran.r-project.org/web/packages/pdftools/pdftools.pdf
#   
#   Basic Handling Errors: http://mazamascience.com/WorkingWithData/?p=912
##

##
# Dependency Libraries
# tm
## wordcloud
## pdftools
##

## Install
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator 
# install.packages("RColorBrewer") # color palettes
# install.packages("pdftools") # process pdf files
## Load
library("tm")
library("wordcloud")
library("pdftools")
# library("wordcloud2")
library("readtext")
library("rlist")

set.seed(8)

## clean all variables
rm(list = ls())

##
#	Working variable definitions
##

##
# NOTE: Modify this variables to your working environment
#
# Working Directories
#   i_workingPath - Initial Working directory
#       i_pdfPath - Path where the pdf documents to process are.
#   i_dataExport  - Directory to save the images with the result wordcloud
#   i_dataExportTags  - Directory to save the frequency tags for the document
##
i_workingPath <- "D:/ricardor/Gain/wwwgain/R_WordCloud/code/data/"
  i_pdfPath <- "documents"
  
i_dataExport <- "D:/ricardor/Gain/wwwgain/R_WordCloud/results/wordclouds/"
i_dataExportTags <- "D:/ricardor/Gain/wwwgain/R_WordCloud/results/tags/"

##
# Add in this section, all the words that you want to exclude in the wordcloud
# verify that they are between quotes and the have a comma separator
##
my_stopwords <- c("e-ii","can","due","will","yes",     # additional user-defined stop words
                  "tm","figs","figure","online", # stop words related to figure captions
                  "rapid", "physical", "review", "communications",  # stop words to references
                  "http","https",
                  "chancla","cacle","rola","chela","orale"  # mexicanisms as an example
                  ) 


setwd(i_workingPath)

##
# Option 1
#   Place the pdf documents to process in an specific path
#   Reads the file names to process
##
filenames <- list.files(path = i_pdfPath, pattern = "*.pdf|*.PDF", full.names = TRUE)

##
# Option 2
#   Add the information of the files that you want to process
#   to the file uri_list.txt
#   Examples of filenames in the file:
#     D:\ricardor_ed\Strategy_Map.pdf
#     https://www.gainhealth.org/wp-content/uploads/2018/02/Nutrition-and-Food-Systems-Lawrence-Haddad-Presentation-February-2018.pdf
#
#   filenames <- scan(file = "./uri_list.txt", what = character(), strip.white = TRUE, blank.lines.skip = TRUE)
##

##
# Option 3
# Files placed in a directory AND information of the pdf files in the uri_list.txt file
# Note: Option 1 needs to be active.
# Reads the txt file
# filenames <- scan(file = "./uri_list.txt", what = character(), strip.white = TRUE, blank.lines.skip = TRUE)
# append the files into the list of files to process
##
filenames <- list.append(filenames, scan(file = "./uri_list.txt", what = "character", strip.white = TRUE, blank.lines.skip = TRUE, sep = "\t"))

# Loop to process all the required files
for (filename in seq_along(filenames) ) {
  ##
  # Reading the content
  # TODO: Add validation file and http existance and reading success
  ##
  the_file <- filenames[filename]
  
  # Windows directory validation Detect \ and change to / Unix directory path structure
  if ( grepl("\\\\", the_file) ) {
    the_file <- gsub("\\\\", "/", the_file)
  }

  # Local File existance Validation
  if ( ! (grepl("http", the_file)) ){
    if ( ! ( file.exists(the_file) ) ) {
      # TODO: Register the filename in an error log
      print(paste("Error finding file: ",the_file))
      next
    }
  }
  
  ## my_pdf <- readPDF(control=list(text="-layout"))(elem=list(uri=the_file), language="en")
  ## text_raw <- my_pdf$content
  ## text_raw <- pdf_text(the_file)
  # Adding Validation for Local Files and External http requests
  text_raw <- tryCatch({
    pdf_text(the_file)
  }, error = function(e){
    #  TODO: Register the filename in an error log
    print(paste("Error processing request: ",the_file, e))
    return (NULL)
  }, # warning = function(war) {
    #  TODO: Register the filename in an error log
    # print(paste("Warning processing request: ",the_file, war))
    #return (NULL)
    
    #  },
    finally = {
    # print(paste("Processed: ",the_file))
  })
  
  # Next cycle if warning or error reading the file
  if (is.null(text_raw)) {
    next
  }
  
  ## text_raw <- text_raw[-c(1:5)] #remove journal header
  ## text_raw <- text_raw[-c(2:17)] #remove author names, affiliations
  ## text_raw <- text_raw[-11] # remove bibliographic reference details
  ## wtext_raw <- text_raw[1:211] #remove list of references
  
  text_corpus <- Corpus(VectorSource(text_raw))
  
  
  corpus_clean <- tm_map(text_corpus, stripWhitespace)
  corpus_clean <- tm_map(corpus_clean, removeNumbers)
  corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
  
  # remove Special characters  
  for(j in seq(corpus_clean)) {
    corpus_clean[[j]] <- gsub("/", " ", corpus_clean[[j]])
    corpus_clean[[j]] <- gsub("@", " ", corpus_clean[[j]])
    corpus_clean[[j]] <- gsub("\\|", " ", corpus_clean[[j]])     
  }
  
  
  # removing the stopwords
  # stopwords English
  corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
  # stopwords French
  corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("fr"))
  # stopwords Spanish
  corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("es"))
  
  corpus_clean <- tm_map(corpus_clean, removeWords, my_stopwords)
  
  # remove puntuactions
  corpus_clean <- tm_map(corpus_clean, removePunctuation)
  
  # corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
  
  
  # wordcloud(corpus_clean, max.words=Inf, random.order=FALSE, scale= c(3, 0.1), colors=brewer.pal(8,"Dark2"))
  
  # readline(prompt="Press [enter] to continue")

    # term-document matrix
  dtm <- TermDocumentMatrix(corpus_clean)
  m <- as.matrix(dtm)
  v <- sort(rowSums(m),decreasing = TRUE)
  d <- data.frame(word = names(v),freq=v)
  
  if ( length(v) > 0 ) {
    
    img_filename <- paste0( i_dataExport,basename(tools::file_path_sans_ext(filenames[filename])), ".png" )
    # File names with special characters ie: don't isn't 
    img_filename <- gsub("%E2%80%99","-",img_filename)
    
    png(filename = img_filename )
    wordcloud(words = d$word, freq = d$freq, min.freq = 1, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8,"Dark2"))
    dev.off()
    
    # File with the Frequency to select the Tags
    tag_filename <- paste0( i_dataExportTags,basename(tools::file_path_sans_ext(filenames[filename])), ".csv" )
    tag_filename <- gsub("%E2%80%99","-",tag_filename)
    
    write.table(head(d,10), file = tag_filename, sep = "," , quote = TRUE , row.names = FALSE)
    
  }
  
  # readline(prompt="Press [enter] to continue")
  
  ## wordcloud2(data = d)
  
  ## readline(prompt="Press [enter] to continue")
  
  
}

